{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"recommender-system.ipynb","collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ncf7sAoOrdt-"},"source":["# Movie recommendations\n","\n","In this notebook, we will take a stab at the task of recommending movies to potential viewers.\n","\n","Specifically, we will start from information of how viewers rated movies they _have_ seen, and predict how they would rate movies that they _have not_ seen, based on how other people with similar movie tastes rated those other movies. This kind of task is called _collaborative filtering_."],"id":"Ncf7sAoOrdt-"},{"cell_type":"markdown","metadata":{"id":"LPiDMc9_secD"},"source":["## Prelude\n","\n","The only new imports are `pandas` and `csv`. Pandas is a very useful library for working with tables (called _dataframes_), in two or more dimensions. While it can sometimes be counter-intuitive, it's very powerful and well worth getting to grips with.\n","\n","Here, we only really use Pandas to read in the dataset CSV files, and to do some elementary preprocessing."],"id":"LPiDMc9_secD"},{"cell_type":"code","metadata":{"id":"accredited-driver"},"source":["import torch, torchtext, numpy as np\n","import pandas as pd, csv\n","from torch import nn, optim\n","from tqdm.auto import tqdm\n","import matplotlib.pyplot as plt\n","import pdb\n","torch.manual_seed(291)\n","np.random.seed(291)"],"id":"accredited-driver","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zt9V8OIypxZG"},"source":["## Data"],"id":"zt9V8OIypxZG"},{"cell_type":"markdown","metadata":{"id":"u8dTVSPktv52"},"source":["We're using the MovieLens 100K dataset. Actually MovieLens has much larger movie ranking datasets, and our model does even better on those, but training takes a bit more time than we have during class."],"id":"u8dTVSPktv52"},{"cell_type":"code","metadata":{"id":"w29zg9ecKUlH"},"source":["!wget http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n","!unzip ml-latest-small.zip"],"id":"w29zg9ecKUlH","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RWfamB_DuAS5"},"source":["There are several CSV files. To warm up, let's look at the `movies.csv` table first."],"id":"RWfamB_DuAS5"},{"cell_type":"code","metadata":{"id":"G4KqltIcpmrM"},"source":["df_movies = pd.read_csv('ml-latest-small/movies.csv')"],"id":"G4KqltIcpmrM","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XM-BahskpmeK"},"source":["df_movies"],"id":"XM-BahskpmeK","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SpGf2z_PuIr9"},"source":["Okay, so each movie has an ID (`movieId`) and other information. Because our focus is on getting recommendations based only on how other users review each movie, we will actually ignore the title, year, and genre this time.\n","\n","What appears to be the first, unlabelled column is actually just the row index, which Pandas keeps explicitly as part of the dataframe structure.\n","\n","Next, let's look at what we are _actually_ interested in, namely the movie ratings."],"id":"SpGf2z_PuIr9"},{"cell_type":"code","metadata":{"id":"JzVrFvLNpmU4"},"source":["df = pd.read_csv('ml-latest-small/ratings.csv')"],"id":"JzVrFvLNpmU4","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qa4dBGwepmK9"},"source":["df"],"id":"Qa4dBGwepmK9","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I6JM0cYjvD5v"},"source":["We have what we need: the user ID, movie ID, and the rating. First, let's look at what the ratings look like. The `df['column']` syntax just selects the column, and `.unique()` collects the unique elements."],"id":"I6JM0cYjvD5v"},{"cell_type":"code","metadata":{"id":"41XlIUV8pmCu"},"source":["df['rating'].unique()"],"id":"41XlIUV8pmCu","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-9N_-z3uwpI9"},"source":["So actually the ratings go up to 5, but they are at 0.5 intervals, so there are 10 of them.\n","\n","But there is something suspicious here. There are 100,835 rows, but 170,875 movies. This means some movie IDs are not mentioned here (this is called the pigeonhole principle), which means that the `movieId` dimension is sparse — but it would be most convenient for us to work with dense embedding tensors.\n","\n","Let's verify how many unique movies we have."],"id":"-9N_-z3uwpI9"},{"cell_type":"code","metadata":{"id":"dZiTYVlIpl6A"},"source":["len(df['movieId'].unique())"],"id":"dZiTYVlIpl6A","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yUWyuGAkxWD_"},"source":["Not even 10,000. We will definitely have to renumber them when we create our dataset.\n","\n","We will also need to convert these dataframes to PyTorch tensors, so we can feed them into our model during training and testing. To do this, we need to first retrieve the actual values in the table using `.values` (this is a NumPy array), and then call `LongTensor()` on that to create a tensor of integers.\n","\n","The `[[ ... ]]` syntax is how one selects multiple columns in Pandas."],"id":"yUWyuGAkxWD_"},{"cell_type":"code","metadata":{"id":"4vB4Ue35CTvV"},"source":["torch.LongTensor(df[['userId', 'movieId']].values)"],"id":"4vB4Ue35CTvV","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I614XyHLyHJO"},"source":["Time to build the dataset class as usual."],"id":"I614XyHLyHJO"},{"cell_type":"markdown","metadata":{"id":"BnkEd2G-q7Jc"},"source":["## Dataset class"],"id":"BnkEd2G-q7Jc"},{"cell_type":"markdown","metadata":{"id":"v6WWByIAyY40"},"source":["This is actually simpler and more straightforward than some datasets we've built before.\n","\n","The only new thing is that we renumber movie and user IDs so that they start from 0 and are contiguous. `u2n` and `m2n` are dictionary comprehensions — just like the list comprehensions we've seen before but these build a lookup table. Finally, `lambda` creates an unnamed function in place: for example, `lambda x: x+x` is a function that doubles its argument."],"id":"v6WWByIAyY40"},{"cell_type":"code","metadata":{"id":"practical-statistics"},"source":["class MovieDataset(torch.utils.data.Dataset):\n","    def __init__(self, fn):\n","        df = pd.read_csv(fn)\n","        u2n = { u: n for n, u in enumerate(df['userId'].unique()) }\n","        m2n = { m: n for n, m in enumerate(df['movieId'].unique()) }\n","        df['userId'] = df['userId'].apply(lambda u: u2n[u])\n","        df['movieId'] = df['movieId'].apply(lambda m: m2n[m])\n","        self.coords = torch.LongTensor(df[['userId','movieId']].values)\n","        self.ratings = torch.FloatTensor(df['rating'].values)\n","        self.n_users = df['userId'].nunique()\n","        self.n_movies = df['movieId'].nunique()\n","\n","    def __len__(self):\n","        return len(self.coords)\n","\n","    def __getitem__(self, i):\n","        return (self.coords[i], self.ratings[i])"],"id":"practical-statistics","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sYhFI6E5zIjt"},"source":["Splitting the dataset is also exactly the same as we've seen before."],"id":"sYhFI6E5zIjt"},{"cell_type":"code","metadata":{"id":"offensive-superintendent"},"source":["ds_full = MovieDataset('ml-latest-small/ratings.csv')\n","n_train = int(0.8 * len(ds_full))\n","n_test = len(ds_full) - n_train\n","rng = torch.Generator().manual_seed(291)\n","ds_train, ds_test = torch.utils.data.random_split(ds_full, [n_train, n_test], rng)"],"id":"offensive-superintendent","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Opmi3UoOvbqO"},"source":["## Recommender model"],"id":"Opmi3UoOvbqO"},{"cell_type":"markdown","metadata":{"id":"GODx7cOCzQI4"},"source":["Now that we have the dataset, we can build our model. Recall that our plan is to create embeddings from both users and movies into the same embedding space, and estimate how much the two embeddings differ by taking the dot product."],"id":"GODx7cOCzQI4"},{"cell_type":"code","metadata":{"id":"historical-acquisition"},"source":["class MovieRecs(nn.Module):\n","    def __init__(self, n_users, n_movies, emb_dim):\n","        super(MovieRecs, self).__init__()\n","        self.user_emb = nn.Embedding(n_users, emb_dim)\n","        self.movie_emb = nn.Embedding(n_movies, emb_dim)\n","        nn.init.xavier_uniform_(self.user_emb.weight)\n","        nn.init.xavier_uniform_(self.movie_emb.weight)\n","    \n","    def forward(self, samples):\n","        users = self.user_emb(samples[:,0])\n","        movies = self.movie_emb(samples[:,1])\n","        return (users * movies).sum(1)"],"id":"historical-acquisition","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JYa-ZIjAzqSh"},"source":["Almost nothing new in the train and test code. The only difference is that `sched.step()` is called _inside_ the iteration loop, rather than outside — this is because we are using the single-cycle learning rate scheduler, which makes smooth adjustments after every minibatch rather than after every epoch.\n","\n","And the dataset is small enough that we don't even need a GPU."],"id":"JYa-ZIjAzqSh"},{"cell_type":"code","metadata":{"id":"political-oregon"},"source":["device = torch.device('cpu')\n","\n","def run_test(model, ldr, crit):\n","    total_loss, total_count = 0, 0\n","    model.eval()\n","    tq_iters = tqdm(ldr, leave=False, desc='test iter')\n","    with torch.no_grad():\n","        for coords, labels in tq_iters:\n","            coords, labels = coords.to(device), labels.to(device)\n","            preds = model(coords)\n","            loss = crit(preds, labels)\n","            total_loss += loss.item() * labels.size(0)\n","            total_count += labels.size(0)\n","            tq_iters.set_postfix({'loss': total_loss/total_count}, refresh=True)\n","    return total_loss / total_count\n","\n","def run_train(model, ldr, crit, opt, sched):\n","    model.train()\n","    total_loss, total_count = 0, 0\n","    tq_iters = tqdm(ldr, leave=False, desc='train iter')\n","    for (coords, labels) in tq_iters:\n","        opt.zero_grad()\n","        coords, labels = coords.to(device), labels.to(device)\n","        preds = model(coords)\n","        loss = crit(preds, labels)\n","        loss.backward()\n","        opt.step()\n","        sched.step()\n","        total_loss += loss.item() * labels.size(0)\n","        total_count += labels.size(0)\n","        tq_iters.set_postfix({'loss': total_loss/total_count}, refresh=True)\n","    return total_loss / total_count\n","\n","def run_all(model, ldr_train, ldr_test, crit, opt, sched, n_epochs=10):\n","    best_loss = np.inf\n","    tq_epochs = tqdm(range(n_epochs), desc='epochs', unit='ep')\n","    for epoch in tq_epochs:\n","        train_loss = run_train(model, ldr_train, crit, opt, sched)\n","        test_loss = run_test(model, ldr_test, crit)\n","        tqdm.write(f'epoch {epoch}   train loss {train_loss:.6f}    test loss {test_loss:.6f}')\n","        if test_loss < best_loss:\n","            best_loss = test_loss\n","            tq_epochs.set_postfix({'bE': epoch, 'bL': best_loss}, refresh=True)"],"id":"political-oregon","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uVO1RF9u0BXR"},"source":["Again, only two things new here.\n","\n","First, we are using mean squared error (MSE) as the loss function. This is only because results on this dataset are normally reported using RMSE (R = root), which PyTorch does not have. We could, of course, easily write an RMSE loss — but minimizing MSE will also minimize the RMSE (because sqrt is strictly increasing), so MSE works just as well.\n","\n","Second, we are using `OneCycleLR()`, mostly because I happened to try that first and it worked well. Often this gives very good results more quickly than other LR schedules, so it's almost always worth trying. This means that we will need to make a `sched.step()` adjustment as described above.\n"],"id":"uVO1RF9u0BXR"},{"cell_type":"code","metadata":{"id":"digital-repository"},"source":["model = MovieRecs(ds_full.n_users, ds_full.n_movies, 20)\n","model.to(device)\n","\n","ldr_train = torch.utils.data.DataLoader(ds_train, batch_size=32, shuffle=True)\n","ldr_test = torch.utils.data.DataLoader(ds_test, batch_size=32)\n","\n","n_epochs = 5\n","\n","crit = nn.MSELoss().to(device)\n","opt = optim.SGD(model.parameters(), lr=1e-6, momentum=0.9)\n","sched = optim.lr_scheduler.OneCycleLR(opt, max_lr=0.1, steps_per_epoch=len(ldr_train), epochs=n_epochs)\n","\n","run_all(model, ldr_train, ldr_test, crit, opt, sched, n_epochs)"],"id":"digital-repository","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W6tnFqmg2Fz7"},"source":["This is already pretty good — try for yourself to evaluate a random, untrained model and compare — but it turns out that we can do even better by making the model slightly more complicated.\n","\n","As we wrote it, the model computes ratings from (user, movie) pairs. But this makes it difficult to account for people who always write bad (or good) reviews, and for movies that are universally considered terrible or amazing. (Or even so terrible that one finds oneself transfixed.)\n","\n","Actually, the model _could_ learn about grumpy reviewers, but it would have to learn this _separately_ for every movie. We can make learning this much easier by adding bias. Let's try."],"id":"W6tnFqmg2Fz7"},{"cell_type":"markdown","metadata":{"id":"meGQh2HivkiK"},"source":["## Recommender model with bias\r\n","\r\n","Let's think what bias means in our case. We want it to learn one value (i.e., the bias offset) separately for every reviewer, and another set of value for the movies. This means that our bias is also an embedding: we index it using the movie (or user) ID, and we get back a single number.\r\n","\r\n","The only fly in the ointment is that this gives us a rank-one tensor with one dimensions, but we can `squeeze()` that extra encapsulation away."],"id":"meGQh2HivkiK"},{"cell_type":"code","metadata":{"id":"nuclear-emerald"},"source":["class MovieRecs(nn.Module):\n","    def __init__(self, n_users, n_movies, emb_dim):\n","        super(MovieRecs, self).__init__()\n","        self.user_emb = nn.Embedding(n_users, emb_dim)\n","        self.user_bias = nn.Embedding(n_users, 1)\n","        self.movie_emb = nn.Embedding(n_movies, emb_dim)\n","        self.movie_bias = nn.Embedding(n_movies, 1)\n","        nn.init.xavier_uniform_(self.user_emb.weight)\n","        nn.init.xavier_uniform_(self.movie_emb.weight)\n","        nn.init.zeros_(self.user_bias.weight)\n","        nn.init.zeros_(self.movie_bias.weight)\n","    \n","    def forward(self, samples):\n","        users = self.user_emb(samples[:,0])\n","        movies = self.movie_emb(samples[:,1])\n","        dot = (users * movies).sum(1)\n","        user_b = self.user_bias(samples[:,0]).squeeze()\n","        movie_b = self.movie_bias(samples[:,1]).squeeze()\n","        return dot + user_b + movie_b"],"id":"nuclear-emerald","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wireless-chambers"},"source":["model = MovieRecs(ds_full.n_users, ds_full.n_movies, 20)\n","model.to(device)\n","\n","ldr_train = torch.utils.data.DataLoader(ds_train, batch_size=32, shuffle=True)\n","ldr_test = torch.utils.data.DataLoader(ds_test, batch_size=32)\n","\n","n_epochs = 5\n","\n","crit = nn.MSELoss().to(device)\n","opt = optim.SGD(model.parameters(), lr=1e-6, momentum=0.9)\n","sched = optim.lr_scheduler.OneCycleLR(opt, max_lr=0.1, steps_per_epoch=len(ldr_train), epochs=n_epochs)\n","\n","run_all(model, ldr_train, ldr_test, crit, opt, sched, n_epochs)"],"id":"wireless-chambers","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JdC0ovap5bTh"},"source":["This converged much faster for me than the previous version and gave me better results.\n","\n","There are a few other simple things I tried, like clamping the predicted rating range in various ways (e.g., sigmoid), regularization, messing with the embedding dimension, and so on. Some of them help a little bit — but they're easy enough to try for yourself.\n"],"id":"JdC0ovap5bTh"}]}
